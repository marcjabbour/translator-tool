#!/usr/bin/env python3
"""
Integration test for complete evaluation workflow.
Tests end-to-end evaluation from API request to database storage.
"""

import asyncio
import httpx
import json
import sys
import time
import uuid
from typing import Dict, Any, List
from datetime import datetime

# Test configuration
BASE_URL = "http://localhost:8000"
TEST_USER_ID = "eval-test-user-123"

# Test data
TEST_LESSON_DATA = {
    "topic": "greetings",
    "level": "beginner",
    "seed": 42
}

TEST_QUIZ_RESPONSES = [
    {"q_index": 0, "value": "hello"},  # Should detect EN_IN_AR error
    {"q_index": 1, "value": "kifak"},   # Should be correct
    {"q_index": 2, "value": "shou"},    # Should detect SPELL_T error
]


async def test_complete_evaluation_workflow():
    """Test the complete evaluation workflow from lesson creation to evaluation."""
    async with httpx.AsyncClient() as client:
        print("ğŸ§ª Starting Complete Evaluation Integration Test")
        print(f"ğŸ”— Testing against: {BASE_URL}")
        print("=" * 60)

        # Create test token
        test_token = await create_test_token(client)
        headers = {
            "Authorization": f"Bearer {test_token}",
            "Content-Type": "application/json"
        }

        try:
            # Step 1: Create a lesson
            print("\nğŸ“– Step 1: Creating test lesson")
            lesson_response = await client.post(
                f"{BASE_URL}/api/v1/story",
                headers=headers,
                json=TEST_LESSON_DATA
            )

            if lesson_response.status_code != 200:
                print(f"âŒ Failed to create lesson: {lesson_response.status_code}")
                print(f"   Response: {lesson_response.text}")
                return False

            lesson_data = lesson_response.json()
            lesson_id = lesson_data["lesson_id"]
            print(f"âœ… Lesson created: {lesson_id}")

            # Step 2: Generate quiz for the lesson
            print("\nğŸ§  Step 2: Generating quiz")
            quiz_response = await client.post(
                f"{BASE_URL}/api/v1/quiz",
                headers=headers,
                json={"lesson_id": lesson_id}
            )

            if quiz_response.status_code != 200:
                print(f"âŒ Failed to generate quiz: {quiz_response.status_code}")
                print(f"   Response: {quiz_response.text}")
                return False

            quiz_data = quiz_response.json()
            quiz_id = quiz_data["quiz_id"]
            questions = quiz_data["questions"]
            print(f"âœ… Quiz generated: {quiz_id} with {len(questions)} questions")

            # Step 3: Prepare evaluation request
            print("\nğŸ” Step 3: Preparing evaluation request")
            evaluation_request = {
                "user_id": TEST_USER_ID,
                "lesson_id": lesson_id,
                "quiz_id": quiz_id,
                "responses": [
                    {"q_index": 0, "value": "hello"},  # EN_IN_AR error
                    {"q_index": 1, "value": "kifak"},  # Correct response
                ]
            }

            # Ensure we only test with available questions
            if len(questions) > 2:
                evaluation_request["responses"].append(
                    {"q_index": 2, "value": "shou"}  # SPELL_T error
                )

            print(f"ğŸ“ Testing with {len(evaluation_request['responses'])} responses")

            # Step 4: Submit for evaluation
            print("\nâš–ï¸  Step 4: Submitting for evaluation")
            eval_start_time = time.time()

            evaluation_response = await client.post(
                f"{BASE_URL}/api/v1/evaluate",
                headers=headers,
                json=evaluation_request
            )

            eval_time = time.time() - eval_start_time

            if evaluation_response.status_code != 200:
                print(f"âŒ Evaluation failed: {evaluation_response.status_code}")
                print(f"   Response: {evaluation_response.text}")
                return False

            eval_data = evaluation_response.json()
            print(f"âœ… Evaluation completed in {eval_time:.2f}s")
            print(f"   Attempt ID: {eval_data['attempt_id']}")
            print(f"   Score: {eval_data['score']:.1%}")

            # Step 5: Validate evaluation results
            print("\nâœ”ï¸  Step 5: Validating evaluation results")
            validation_passed = await validate_evaluation_results(eval_data)

            if not validation_passed:
                return False

            # Step 6: Test error classification accuracy
            print("\nğŸ¯ Step 6: Testing error classification accuracy")
            accuracy_passed = await test_error_classification_accuracy(eval_data)

            if not accuracy_passed:
                return False

            # Step 7: Test database storage
            print("\nğŸ’¾ Step 7: Validating database storage")
            storage_passed = await test_database_storage(client, headers, eval_data["attempt_id"])

            if not storage_passed:
                return False

            # Step 8: Test performance requirements
            print("\nâš¡ Step 8: Testing performance requirements")
            performance_passed = await test_performance_requirements(client, headers, evaluation_request)

            if not performance_passed:
                return False

            print("\n" + "=" * 60)
            print("ğŸ‰ ALL INTEGRATION TESTS PASSED!")
            print("=" * 60)
            return True

        except Exception as e:
            print(f"\nğŸ’¥ Integration test failed: {e}")
            import traceback
            traceback.print_exc()
            return False


async def validate_evaluation_results(eval_data: Dict[str, Any]) -> bool:
    """Validate the structure and content of evaluation results."""
    print("   ğŸ” Validating result structure...")

    # Check required fields
    required_fields = ["attempt_id", "score", "feedback"]
    for field in required_fields:
        if field not in eval_data:
            print(f"   âŒ Missing required field: {field}")
            return False

    # Validate attempt_id format (should be UUID)
    try:
        uuid.UUID(eval_data["attempt_id"])
        print("   âœ… Valid attempt ID format")
    except ValueError:
        print(f"   âŒ Invalid attempt ID format: {eval_data['attempt_id']}")
        return False

    # Validate score range
    score = eval_data["score"]
    if not (0.0 <= score <= 1.0):
        print(f"   âŒ Score out of range: {score}")
        return False
    print(f"   âœ… Valid score: {score:.1%}")

    # Validate feedback structure
    feedback = eval_data["feedback"]
    if not isinstance(feedback, list):
        print("   âŒ Feedback should be a list")
        return False

    for i, fb in enumerate(feedback):
        required_fb_fields = ["q_index", "ok", "errors"]
        for field in required_fb_fields:
            if field not in fb:
                print(f"   âŒ Missing feedback field {field} in question {i}")
                return False

        # Validate errors structure
        if not isinstance(fb["errors"], list):
            print(f"   âŒ Errors should be a list in question {i}")
            return False

        for error in fb["errors"]:
            if not isinstance(error, dict) or "type" not in error or "token" not in error:
                print(f"   âŒ Invalid error structure in question {i}")
                return False

    print(f"   âœ… Valid feedback structure for {len(feedback)} questions")
    return True


async def test_error_classification_accuracy(eval_data: Dict[str, Any]) -> bool:
    """Test the accuracy of error classification."""
    print("   ğŸ¯ Testing error classification accuracy...")

    feedback = eval_data["feedback"]
    correct_classifications = 0
    total_responses = len(feedback)

    expected_results = [
        {"correct": False, "error_types": ["EN_IN_AR"]},  # "hello" response
        {"correct": True, "error_types": []},             # "kifak" response
        {"correct": False, "error_types": ["SPELL_T"]},   # "shou" response (if present)
    ]

    for i, fb in enumerate(feedback):
        if i < len(expected_results):
            expected = expected_results[i]
            actual_correct = fb["ok"]
            actual_errors = [err["type"] for err in fb["errors"]]

            print(f"     Q{i+1}: Expected correct={expected['correct']}, Got correct={actual_correct}")
            print(f"         Expected errors={expected['error_types']}, Got errors={actual_errors}")

            # Check correctness classification
            if actual_correct == expected["correct"]:
                # Check error type detection
                if expected["error_types"]:
                    # Should have detected at least one expected error type
                    if any(err_type in actual_errors for err_type in expected["error_types"]):
                        correct_classifications += 1
                        print(f"     âœ… Correct classification for Q{i+1}")
                    else:
                        print(f"     âŒ Missed expected error types for Q{i+1}")
                else:
                    # Should not have detected errors (or minor ones are acceptable)
                    correct_classifications += 1
                    print(f"     âœ… Correct classification for Q{i+1}")
            else:
                print(f"     âŒ Incorrect correctness classification for Q{i+1}")

    accuracy = correct_classifications / total_responses
    print(f"   ğŸ“Š Classification accuracy: {accuracy:.1%} ({correct_classifications}/{total_responses})")

    # Require at least 80% accuracy
    if accuracy >= 0.8:
        print("   âœ… Meets â‰¥80% accuracy requirement")
        return True
    else:
        print("   âŒ Below 80% accuracy requirement")
        return False


async def test_database_storage(client: httpx.AsyncClient, headers: Dict[str, str], attempt_id: str) -> bool:
    """Test that evaluation results are properly stored in database."""
    print("   ğŸ’¾ Testing database storage...")

    # Note: In a real test, you'd query the database directly
    # For this integration test, we'll verify the attempt_id is valid
    # and that the evaluation endpoint worked (which implies storage)

    try:
        # Validate UUID format (indicates proper database ID generation)
        uuid.UUID(attempt_id)
        print(f"   âœ… Valid attempt ID generated: {attempt_id}")

        # In a complete test, you might:
        # - Query the attempts table directly
        # - Query the errors table for error records
        # - Verify data integrity

        return True

    except ValueError:
        print(f"   âŒ Invalid attempt ID format: {attempt_id}")
        return False


async def test_performance_requirements(
    client: httpx.AsyncClient,
    headers: Dict[str, str],
    evaluation_request: Dict[str, Any]
) -> bool:
    """Test performance requirements for evaluation."""
    print("   âš¡ Testing performance requirements...")

    # Test single request latency
    start_time = time.time()
    response = await client.post(
        f"{BASE_URL}/api/v1/evaluate",
        headers=headers,
        json=evaluation_request
    )
    single_latency = time.time() - start_time

    if response.status_code != 200:
        print(f"   âŒ Performance test request failed: {response.status_code}")
        return False

    print(f"   ğŸ“Š Single request latency: {single_latency:.3f}s")

    # Requirement: Should complete within 10 seconds
    if single_latency > 10.0:
        print(f"   âŒ Latency exceeds 10s requirement: {single_latency:.3f}s")
        return False

    print("   âœ… Latency within acceptable range")

    # Test concurrent requests (basic load test)
    print("   ğŸ”„ Testing concurrent evaluation requests...")
    concurrent_requests = 5
    tasks = []

    for i in range(concurrent_requests):
        # Modify user_id slightly to avoid conflicts
        modified_request = evaluation_request.copy()
        modified_request["user_id"] = f"{evaluation_request['user_id']}-{i}"

        task = client.post(
            f"{BASE_URL}/api/v1/evaluate",
            headers=headers,
            json=modified_request
        )
        tasks.append(task)

    start_time = time.time()
    responses = await asyncio.gather(*tasks, return_exceptions=True)
    concurrent_time = time.time() - start_time

    successful_responses = sum(1 for r in responses if not isinstance(r, Exception) and r.status_code == 200)
    throughput = successful_responses / concurrent_time

    print(f"   ğŸ“Š Concurrent requests: {concurrent_requests}")
    print(f"   ğŸ“Š Successful: {successful_responses}")
    print(f"   ğŸ“Š Total time: {concurrent_time:.3f}s")
    print(f"   ğŸ“Š Throughput: {throughput:.1f} req/sec")

    # Basic requirement: Should handle concurrent requests without major failures
    if successful_responses < concurrent_requests * 0.8:  # Allow 20% failure rate
        print(f"   âŒ Too many failed concurrent requests")
        return False

    print("   âœ… Concurrent request handling acceptable")
    return True


async def create_test_token(client: httpx.AsyncClient) -> str:
    """Create a test JWT token for authentication."""
    try:
        import jwt
        import time

        payload = {
            "sub": TEST_USER_ID,
            "user_id": TEST_USER_ID,
            "exp": int(time.time()) + 3600,  # 1 hour
            "iat": int(time.time()),
            "iss": "translator-tool-test"
        }

        # Use the same secret as the server (for testing only)
        token = jwt.encode(payload, "your-jwt-secret-key", algorithm="HS256")
        print(f"âœ… Created test token for user: {TEST_USER_ID}")
        return token

    except Exception as e:
        print(f"âŒ Failed to create test token: {e}")
        sys.exit(1)


async def check_server_health():
    """Check if the server is running."""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{BASE_URL}/health")
            if response.status_code == 200:
                print("âœ… Server is running")
                return True
            else:
                print(f"âŒ Server health check failed: {response.status_code}")
                return False
    except Exception as e:
        print(f"âŒ Cannot connect to server: {e}")
        print(f"   Make sure the server is running on {BASE_URL}")
        return False


async def main():
    """Main integration test function."""
    print("ğŸš€ Evaluation Service Integration Test Suite")
    print(f"ğŸ”— Testing against: {BASE_URL}")
    print("ğŸ¯ Goals: End-to-end workflow validation, â‰¥80% accuracy, performance")
    print()

    # Check server health first
    if not await check_server_health():
        print("\nğŸ’¡ To start the server, run:")
        print("   cd /path/to/your/backend")
        print("   uvicorn app.main:app --reload")
        sys.exit(1)

    # Run integration tests
    success = await test_complete_evaluation_workflow()

    if success:
        print("\nğŸ‰ All integration tests passed!")
        print("âœ… Evaluation service is ready for production")
        sys.exit(0)
    else:
        print("\nâŒ Integration tests failed!")
        print("ğŸ”§ Please review and fix issues before deploying")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())